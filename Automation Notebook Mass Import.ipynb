{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automation Script Overhaul \n",
    "For automating the parsing, transformation, and geographic file creation from .GPX<br><br>\n",
    "Work taken from the script by Isias (ler_gpx.py) and workflow by Simone. Further developments by Kyle & Isais. <br><br>\n",
    "Data collection must be done with Locus Map 4.x or formatted similarly to GPX files exportd by Locus Map 4 for this script to work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 1\n",
    "This section takes the raw GPX files and makes them ready for QGIS and statistical analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gpxpy\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import os\n",
    "from os import mkdir\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Create a dictionary with all gpx files in dictionary / Crie um dicionário com todos os arquivos gpx no dicionário\n",
    "\n",
    "gpxDict = dict()\n",
    "\n",
    "for file in os.listdir():\n",
    "    if file.endswith('.gpx'):\n",
    "       gpxDict[file] = 'file_'+file\n",
    "gpxDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Daily Data\n",
    "This next for loop / cell will create files with the data sorted by day and must be run before creaating data files by scan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gpxpy\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import os\n",
    "from os import mkdir\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Create a dictionary with all gpx files in dictionary / Crie um dicionário com todos os arquivos gpx no dicionário\n",
    "\n",
    "gpxDict = dict()\n",
    "\n",
    "for file in os.listdir():\n",
    "    if file.endswith('.gpx'):\n",
    "       gpxDict[file] = 'file_'+file\n",
    "\n",
    "# Run for loop to cover every gpx file in directory / Execute o loop para cobrir todos os arquivos gpx no diretório\n",
    "\n",
    "for i in gpxDict:\n",
    "    \n",
    "    # Open and read in the .gpx to a dataframe / Abra e leia no .gpx para um dataframe\n",
    "    gpxCurrent = i\n",
    "    gpxCurrent = open(gpxCurrent)\n",
    "    gpxCurrent = gpxpy.parse(gpxCurrent)\n",
    "    gpxCurrent = gpxCurrent.to_xml()\n",
    "    df = pd.read_xml(gpxCurrent)\n",
    "\n",
    "    # Ask for observer, group, climate conditions / Pergunte por observador, grupo, condições climáticas\n",
    "    observer = input('Input for '+i+': Observer/Observador? ')\n",
    "    group = input('Input for '+i+': Group/Grupo? (if both, mark 0) ') # or leave blank?\n",
    "    weather = input('Input for '+i+': Weather conditions/Condição do clima? ')\n",
    "    \n",
    "    # Remove unecessary columns / Remova colunas desnecessárias\n",
    "    df.pop('desc')\n",
    "    df.pop('hdop')\n",
    "    df.pop('time')\n",
    "    df = df.drop(index=0)\n",
    "\n",
    "    # Reorganize columns / Reorganizar colunas\n",
    "    shiftPos = df.pop('name')\n",
    "    df.insert(0, 'name', shiftPos)\n",
    "\n",
    "    # Insert user input columns if they have a value / Insira colunas de entrada do usuário se elas tiverem um valor\n",
    "    if observer:\n",
    "        df.insert(loc=1, column='observer', value=observer, allow_duplicates=True)\n",
    "    if group:\n",
    "        df.insert(loc=1, column='group', value=group, allow_duplicates=True)\n",
    "    if weather:\n",
    "        df.insert(loc=1, column='weather', value=weather, allow_duplicates=True)\n",
    "\n",
    "    # Split 'name' into date, time, and observations / Dividir 'nome' em data, hora e observações\n",
    "    date = df['name'].str[:10]\n",
    "    df.insert(loc=0, column='date', value=date, allow_duplicates=True)\n",
    "\n",
    "    time = df['name'].str[11:19]\n",
    "    df.insert(loc=1, column='time', value=time, allow_duplicates=True)\n",
    "\n",
    "    obs = df['name'].str[19:]\n",
    "    df.insert(loc=2, column='observations', value=obs, allow_duplicates=True)\n",
    "    # Remove whitespace from observations column / Remover espaço em branco da coluna de observações\n",
    "    df['observations'] = df['observations'].str.strip()\n",
    "\n",
    "    df.pop('name')\n",
    "\n",
    "    \n",
    "\n",
    "    # Setup time variables for scan labeling / Variáveis de tempo de configuração para rotulagem de digitalização\n",
    "    scanStart = df.at[1,'time']\n",
    "    scanStart = datetime.strptime(scanStart,'%H:%M:%S')\n",
    "    scanEnd = scanStart + timedelta(minutes=20)\n",
    "    df.insert(loc=2, column='scan', value=0, allow_duplicates=True)\n",
    "    scanMins = 20\n",
    "\n",
    "    # Create list for temporary storage of scan ID's / Criar lista para armazenamento temporário de IDs de digitalização\n",
    "    scanNum = []\n",
    "\n",
    "    # Loop to check each time against the times for each day and assign scan ID\n",
    "    # Faça um loop para verificar cada vez em relação aos horários de cada dia e atribuir a ID de verificação\n",
    "    for row in df['time']:\n",
    "        row = datetime.strptime(row,'%H:%M:%S')\n",
    "        if scanStart <= row <= scanEnd:\n",
    "            scanNum.append('1')\n",
    "        elif (scanStart + timedelta(minutes=scanMins*2)) <= row <= (scanEnd + timedelta(minutes=scanMins*2)):\n",
    "            scanNum.append('2')\n",
    "        elif (scanStart + timedelta(minutes=scanMins*4)) <= row <= (scanEnd + timedelta(minutes=scanMins*4)):\n",
    "            scanNum.append('3')\n",
    "        elif (scanStart + timedelta(minutes=scanMins*6)) <= row <= (scanEnd + timedelta(minutes=scanMins*6)):\n",
    "            scanNum.append('4')\n",
    "        elif (scanStart + timedelta(minutes=scanMins*8)) <= row <= (scanEnd + timedelta(minutes=scanMins*8)):\n",
    "            scanNum.append('5')\n",
    "        elif (scanStart + timedelta(minutes=scanMins*10)) <= row <= (scanEnd + timedelta(minutes=scanMins*10)):\n",
    "            scanNum.append('6')\n",
    "        elif (scanStart + timedelta(minutes=scanMins*12)) <= row <= (scanEnd + timedelta(minutes=scanMins*12)):\n",
    "            scanNum.append('7')\n",
    "        elif (scanStart + timedelta(minutes=scanMins*14)) <= row <= (scanEnd + timedelta(minutes=scanMins*14)):\n",
    "            scanNum.append('8')\n",
    "        elif (scanStart + timedelta(minutes=scanMins*16)) <= row <= (scanEnd + timedelta(minutes=scanMins*16)):\n",
    "            scanNum.append('9')\n",
    "        elif (scanStart + timedelta(minutes=scanMins*18)) <= row <= (scanEnd + timedelta(minutes=scanMins*18)):\n",
    "            scanNum.append('10')\n",
    "        elif (scanStart + timedelta(minutes=scanMins*20)) <= row <= (scanEnd + timedelta(minutes=scanMins*20)):\n",
    "            scanNum.append('11')\n",
    "        elif (scanStart + timedelta(minutes=scanMins*22)) <= row <= (scanEnd + timedelta(minutes=scanMins*22)):\n",
    "            scanNum.append('12')\n",
    "        elif (scanStart + timedelta(minutes=scanMins*24)) <= row <= (scanEnd + timedelta(minutes=scanMins*24)):\n",
    "            scanNum.append('13')\n",
    "        elif (scanStart + timedelta(minutes=scanMins*26)) <= row <= (scanEnd + timedelta(minutes=scanMins*26)):\n",
    "            scanNum.append('14')\n",
    "        elif (scanStart + timedelta(minutes=scanMins*28)) <= row <= (scanEnd + timedelta(minutes=scanMins*28)):\n",
    "            scanNum.append('15')\n",
    "        elif (scanStart + timedelta(minutes=scanMins*30)) <= row <= (scanEnd + timedelta(minutes=scanMins*30)):\n",
    "            scanNum.append('16')\n",
    "\n",
    "        # If no times fit, apply N/A / Se nenhum tempo se encaixar, aplique N/A\n",
    "        else:\n",
    "            scanNum.append('N/A') \n",
    "\n",
    "    # Apply scan ID list to the dataframe / Aplicar lista de IDs de varredura ao dataframe\n",
    "    df['scan'] = scanNum\n",
    "\n",
    "    # Create lists to store observations / Crie listas para armazenar observações\n",
    "    scanAgeSex = []\n",
    "    scanStrata = []\n",
    "    scanBehaviour = []\n",
    "\n",
    "    # Run loop to parse observations and store in lists / Executar loop para analisar observações e armazenar em listas\n",
    "    for row in df['observations']:\n",
    "            # Check the two character codes first to avoid conflicts and any misidentified lines such as 'mf' and 'ff' going to 'm' and 'f'\n",
    "            # Verifique os dois códigos de caracteres primeiro para evitar conflitos e quaisquer linhas mal identificadas, como 'mf' e 'ff' indo para 'm' e 'f'\n",
    "            if row[:2] == 'j1':\n",
    "                    # Append relevant values to appropriate lists / Anexar valores relevantes a listas apropriadas\n",
    "                    scanAgeSex.append('j1')\n",
    "                    scanStrata.append(row[2:3])\n",
    "                    if row[3:]:\n",
    "                            scanBehaviour.append(row[3:])\n",
    "                    else:\n",
    "                            scanBehaviour.append('lof')\n",
    "            elif row[:2] == 'j2':\n",
    "                    scanAgeSex.append('j2')\n",
    "                    scanStrata.append(row[2:3])\n",
    "                    if row[3:]:\n",
    "                            scanBehaviour.append(row[3:])\n",
    "                    else:\n",
    "                            scanBehaviour.append('lof')\n",
    "            elif row[:2] == 'j3':\n",
    "                    scanAgeSex.append('j3')\n",
    "                    scanStrata.append(row[2:3])\n",
    "                    if row[3:]:\n",
    "                            scanBehaviour.append(row[3:])\n",
    "                    else:\n",
    "                            scanBehaviour.append('lof')\n",
    "            elif row[:2] == 'ff':\n",
    "                    scanAgeSex.append('ff')\n",
    "                    scanStrata.append(row[2:3])\n",
    "                    if row[3:]:\n",
    "                            scanBehaviour.append(row[3:])\n",
    "                    else:\n",
    "                            scanBehaviour.append('lof')\n",
    "            elif row[:2] == 'mf':\n",
    "                    scanAgeSex.append('mf')\n",
    "                    scanStrata.append(row[2:3])\n",
    "                    if row[3:]:\n",
    "                            scanBehaviour.append(row[3:])\n",
    "                    else:\n",
    "                            scanBehaviour.append('lof')\n",
    "            elif row[:2] == 'sa':\n",
    "                    scanAgeSex.append('sa')\n",
    "                    scanStrata.append(row[2:3])\n",
    "                    if row[3:]:\n",
    "                            scanBehaviour.append(row[3:])\n",
    "                    else:\n",
    "                            scanBehaviour.append('lof')\n",
    "            elif row[:2] == 'ni':\n",
    "                    scanAgeSex.append('ni')\n",
    "                    scanStrata.append(row[2:3])\n",
    "                    if row[3:]:\n",
    "                            scanBehaviour.append(row[3:])\n",
    "                    else:\n",
    "                            scanBehaviour.append('lof')\n",
    "            elif row[:1] == 'f':\n",
    "                    scanAgeSex.append('f')\n",
    "                    scanStrata.append(row[1:2])\n",
    "                    if row[3:]:\n",
    "                            scanBehaviour.append(row[2:])\n",
    "                    else:\n",
    "                            scanBehaviour.append('lof')\n",
    "            elif row[:1] == 'm':\n",
    "                    scanAgeSex.append('m')\n",
    "                    scanStrata.append(row[1:2])\n",
    "                    if row[3:]:\n",
    "                            scanBehaviour.append(row[2:])\n",
    "                    else:\n",
    "                            scanBehaviour.append('lof')\n",
    "            elif row[:3] == 'ago':\n",
    "                    scanAgeSex.append('ago')\n",
    "                    scanStrata.append('')\n",
    "                    scanBehaviour.append('')\n",
    "            else:\n",
    "                    scanAgeSex.append('ERROR')\n",
    "                    scanStrata.append('ERROR')\n",
    "                    scanBehaviour.append('ERROR')\n",
    "\n",
    "    # Write lists to columns in the current dataframe / Gravar listas em colunas no dataframe atual\n",
    "    df.insert(loc=2, column='strata', value=scanStrata, allow_duplicates=True)\n",
    "    df.insert(loc=2, column='behaviour', value=scanBehaviour, allow_duplicates=True)\n",
    "    df.insert(loc=2, column='age/sex', value=scanAgeSex, allow_duplicates=True)\n",
    "\n",
    "    # Make geographic and set CRS / Faça geográfica e defina CRS\n",
    "    gdf = gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(df.lon, df.lat)) \n",
    "    gdf = gdf.set_crs('EPSG:4326')\n",
    "\n",
    "    # Export gdf into gpkg / Exportar gdf para gpkg\n",
    "    gdf.to_file('daily_data.gpkg', driver=\"GPKG\", layer=i[:-4])\n",
    "\n",
    "    # Check and create save directory for csv files / Verifique e crie um diretório de salvamento para arquivos csv\n",
    "    savePath = './csvDayFiles'\n",
    "    isDir = os.path.isdir(savePath)\n",
    "    if isDir == False:\n",
    "        mkdir('csvDayFiles')\n",
    "    \n",
    "    # Save to csv / Salvar em csv\n",
    "    gdf.to_csv('csvDayFiles/'+i[:-4]+'.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Scan by scan data\n",
    "This next cell will take the daily data (after manual cleanup preferably) and create new data files organized by date and scan number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "False",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/Nextcloud/Monkey Research/Data_Work/automacao/venvScanAutomation/lib/python3.8/site-packages/pandas/core/indexes/base.py:3621\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3620\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 3621\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mget_loc(casted_key)\n\u001b[1;32m   3622\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/Nextcloud/Monkey Research/Data_Work/automacao/venvScanAutomation/lib/python3.8/site-packages/pandas/_libs/index.pyx:136\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/Nextcloud/Monkey Research/Data_Work/automacao/venvScanAutomation/lib/python3.8/site-packages/pandas/_libs/index.pyx:163\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5198\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5206\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: False",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m/home/kyle/Nextcloud/Monkey Research/Data_Work/automacao/Automation Notebook Mass Import.ipynb Cell 7'\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/kyle/Nextcloud/Monkey%20Research/Data_Work/automacao/Automation%20Notebook%20Mass%20Import.ipynb#ch0000016?line=9'>10</a>\u001b[0m df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(\u001b[39m'\u001b[39m\u001b[39mcsvDayFiles/\u001b[39m\u001b[39m'\u001b[39m\u001b[39m+\u001b[39mfile)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/kyle/Nextcloud/Monkey%20Research/Data_Work/automacao/Automation%20Notebook%20Mass%20Import.ipynb#ch0000016?line=10'>11</a>\u001b[0m df\u001b[39m.\u001b[39mpop(\u001b[39m'\u001b[39m\u001b[39mUnnamed: 0\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/kyle/Nextcloud/Monkey%20Research/Data_Work/automacao/Automation%20Notebook%20Mass%20Import.ipynb#ch0000016?line=12'>13</a>\u001b[0m scan1 \u001b[39m=\u001b[39m df[df[\u001b[39m'\u001b[39;49m\u001b[39mscan\u001b[39;49m\u001b[39m'\u001b[39;49m \u001b[39m==\u001b[39;49m \u001b[39m'\u001b[39;49m\u001b[39m1\u001b[39;49m\u001b[39m'\u001b[39;49m]]\n",
      "File \u001b[0;32m~/Nextcloud/Monkey Research/Data_Work/automacao/venvScanAutomation/lib/python3.8/site-packages/pandas/core/frame.py:3505\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3503\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mnlevels \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   3504\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 3505\u001b[0m indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcolumns\u001b[39m.\u001b[39;49mget_loc(key)\n\u001b[1;32m   3506\u001b[0m \u001b[39mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   3507\u001b[0m     indexer \u001b[39m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/Nextcloud/Monkey Research/Data_Work/automacao/venvScanAutomation/lib/python3.8/site-packages/pandas/core/indexes/base.py:3623\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3621\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine\u001b[39m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3622\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m-> 3623\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[1;32m   3624\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m   3625\u001b[0m     \u001b[39m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3626\u001b[0m     \u001b[39m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3627\u001b[0m     \u001b[39m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3628\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: False"
     ]
    }
   ],
   "source": [
    "# export each scan as individual file in gpkg and csv\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import os\n",
    "from os import mkdir\n",
    "\n",
    "\n",
    "for file in os.listdir('csvDayFiles'):\n",
    "    if file.endswith('.csv'):\n",
    "        df = pd.read_csv('csvDayFiles/'+file)\n",
    "        df.pop('Unnamed: 0')\n",
    "\n",
    "        for i,  g in df.groupby('scan'):\n",
    "            \n",
    "\n",
    "\n",
    "# seperate by scan into new df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Section 2\n",
    "Analysis of scans, analyze all of this for every individual scan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find centroid of each scan (collect geometries, find centroid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distance of each point/animal to centroid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# points to polygons for area (ha) of group spread (convex hull)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distance between each centroid in temporal order\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subgroups/cluster analysis, find clusters on eah scan and distance from each sub-centroid to main group centroid\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Export Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export data frame to gpkg for use in QGIS / Exportar quadro de dados para gpkg para uso no QGIS\n",
    "gdf.to_file('gdf.gpkg', driver=\"GPKG\", layer='gdfExport')\n",
    "gdf.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf.plot()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "721752d912caa1798787a48d49dc55f567da66e2c166f2b94012337a164a3121"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('venvScanAutomation': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
