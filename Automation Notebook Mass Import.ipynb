{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automation Script Overhaul \n",
    "For automating the parsing, transformation, and geographic file creation from .GPX<br><br>\n",
    "Work taken from the script by Isias (ler_gpx.py) and workflow by Simone. Further developments by Kyle & Isais. <br><br>\n",
    "Data collection must be done with Locus Map 4.x or formatted similarly to GPX files exportd by Locus Map 4 for this script to work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 1\n",
    "This section takes the raw GPX files and makes them ready for QGIS and statistical analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gpxpy\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import os\n",
    "from os import mkdir\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Create a dictionary with all gpx files in dictionary / Crie um dicionário com todos os arquivos gpx no dicionário\n",
    "\n",
    "gpxDict = dict()\n",
    "\n",
    "for file in os.listdir():\n",
    "    if file.endswith('.gpx'):\n",
    "       gpxDict[file] = 'file_'+file\n",
    "gpxDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Daily Data\n",
    "This next for loop / cell will create files with the data sorted by day and must be run before creaating data files by scan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gpxpy\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import warnings\n",
    "import os\n",
    "from os import mkdir\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import spatialFunctions\n",
    "from spatialFunctions import *\n",
    "\n",
    "# Remove warning message for future warnings / Remover mensagem de aviso para avisos futuros\n",
    "warnings.filterwarnings(action='ignore',category=FutureWarning)\n",
    "# Suppress warning for setting with copy, non-issue here / Suprimir aviso para configuração com cópia, não é problema aqui\n",
    "pd.set_option('mode.chained_assignment',None)\n",
    "\n",
    "# Get user decision for input fields and state the variables / Obtenha a decisão do usuário para campos de entrada e indique as variáveis\n",
    "userInput = input('To add observer, group, and weather information for each day type \\'yes\\'. Otherwise hit escape')\n",
    "observer = ''\n",
    "group = ''\n",
    "weather = ''\n",
    "\n",
    "# Create a dictionary with all gpx files in dictionary / Crie um dicionário com todos os arquivos gpx no dicionário\n",
    "\n",
    "gpxDict = dict()\n",
    "\n",
    "for file in os.listdir():\n",
    "    if file.endswith('.gpx'):\n",
    "       gpxDict[file] = 'file_'+file\n",
    "\n",
    "# Run for loop to cover every gpx file in directory / Execute o loop para cobrir todos os arquivos gpx no diretório\n",
    "\n",
    "for i in gpxDict:\n",
    "    \n",
    "    # Open and read in the .gpx to a dataframe / Abra e leia no .gpx para um dataframe\n",
    "    gpxCurrent = i\n",
    "    gpxCurrent = open(gpxCurrent)\n",
    "    gpxCurrent = gpxpy.parse(gpxCurrent)\n",
    "    gpxCurrent = gpxCurrent.to_xml()\n",
    "    df = pd.read_xml(gpxCurrent)\n",
    "\n",
    "    # Ask for observer, group, climate conditions / Pergunte por observador, grupo, condições climáticas\n",
    "    if userInput:\n",
    "        observer = input('Input for '+i+': Observer/Observador? ')\n",
    "        group = input('Input for '+i+': Group/Grupo? (if both, mark 0) ') # or leave blank?\n",
    "        weather = input('Input for '+i+': Weather conditions/Condição do clima? ')\n",
    "    \n",
    "    # Remove unecessary columns / Remova colunas desnecessárias\n",
    "    df.pop('desc')\n",
    "    df.pop('time')\n",
    "    if 'hdop' in df.columns:\n",
    "        df.pop('hdop')\n",
    "    df = df.drop(index=0)\n",
    "\n",
    "    # Reorganize columns / Reorganizar colunas\n",
    "    shiftPos = df.pop('name')\n",
    "    df.insert(0, 'name', shiftPos)\n",
    "\n",
    "    # Insert user input columns if they have a value / Insira colunas de entrada do usuário se elas tiverem um valor\n",
    "    if observer:\n",
    "        df.insert(loc=1, column='observer', value=observer, allow_duplicates=True)\n",
    "    if group:\n",
    "        df.insert(loc=1, column='group', value=group, allow_duplicates=True)\n",
    "    if weather:\n",
    "        df.insert(loc=1, column='weather', value=weather, allow_duplicates=True)\n",
    "\n",
    "    # Split 'name' into date, time, and observations / Dividir 'nome' em data, hora e observações\n",
    "    date = df['name'].str[:10]\n",
    "    df.insert(loc=0, column='date', value=date, allow_duplicates=True)\n",
    "\n",
    "    time = df['name'].str[11:19]\n",
    "    df.insert(loc=1, column='time', value=time, allow_duplicates=True)\n",
    "\n",
    "    obs = df['name'].str[19:]\n",
    "    df.insert(loc=2, column='obs', value=obs, allow_duplicates=True)\n",
    "    # Remove whitespace from observations column / Remover espaço em branco da coluna de observações\n",
    "    df['obs'] = df['obs'].str.strip()\n",
    "\n",
    "    df.pop('name')\n",
    "\n",
    "    timeScan(df)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    # # Create list for temporary storage of scan ID's / Criar lista para armazenamento temporário de IDs de digitalização\n",
    "    # scanNum = []\n",
    "\n",
    "    # # Loop to check each time against the times for each day and assign scan ID\n",
    "    # # Faça um loop para verificar cada vez em relação aos horários de cada dia e atribuir a ID de verificação\n",
    "    # for row in df['time']:\n",
    "    #     row = datetime.strptime(row,'%H:%M:%S')\n",
    "    #     if scanStart <= row <= scanEnd:\n",
    "    #         scanNum.append('1')\n",
    "    #     elif (scanStart + timedelta(minutes=scanMins*2-bufferMins)) <= row <= (scanEnd + timedelta(minutes=scanMins*2+bufferMins)):\n",
    "    #         scanNum.append('2')\n",
    "    #     elif (scanStart + timedelta(minutes=scanMins*4-bufferMins)) <= row <= (scanEnd + timedelta(minutes=scanMins*4+bufferMins)):\n",
    "    #         scanNum.append('3')\n",
    "    #     elif (scanStart + timedelta(minutes=scanMins*6-bufferMins)) <= row <= (scanEnd + timedelta(minutes=scanMins*6+bufferMins)):\n",
    "    #         scanNum.append('4')\n",
    "    #     elif (scanStart + timedelta(minutes=scanMins*8-bufferMins)) <= row <= (scanEnd + timedelta(minutes=scanMins*8+bufferMins)):\n",
    "    #         scanNum.append('5')\n",
    "    #     elif (scanStart + timedelta(minutes=scanMins*10-bufferMins)) <= row <= (scanEnd + timedelta(minutes=scanMins*10+bufferMins)):\n",
    "    #         scanNum.append('6')\n",
    "    #     elif (scanStart + timedelta(minutes=scanMins*12-bufferMins)) <= row <= (scanEnd + timedelta(minutes=scanMins*12+bufferMins)):\n",
    "    #         scanNum.append('7')\n",
    "    #     elif (scanStart + timedelta(minutes=scanMins*14-bufferMins)) <= row <= (scanEnd + timedelta(minutes=scanMins*14+bufferMins)):\n",
    "    #         scanNum.append('8')\n",
    "    #     elif (scanStart + timedelta(minutes=scanMins*16-bufferMins)) <= row <= (scanEnd + timedelta(minutes=scanMins*16+bufferMins)):\n",
    "    #         scanNum.append('9')\n",
    "    #     elif (scanStart + timedelta(minutes=scanMins*18-bufferMins)) <= row <= (scanEnd + timedelta(minutes=scanMins*18+bufferMins)):\n",
    "    #         scanNum.append('10')\n",
    "    #     elif (scanStart + timedelta(minutes=scanMins*20-bufferMins)) <= row <= (scanEnd + timedelta(minutes=scanMins*20+bufferMins)):\n",
    "    #         scanNum.append('11')\n",
    "    #     elif (scanStart + timedelta(minutes=scanMins*22-bufferMins)) <= row <= (scanEnd + timedelta(minutes=scanMins*22+bufferMins)):\n",
    "    #         scanNum.append('12')\n",
    "    #     elif (scanStart + timedelta(minutes=scanMins*24-bufferMins)) <= row <= (scanEnd + timedelta(minutes=scanMins*24+bufferMins)):\n",
    "    #         scanNum.append('13')\n",
    "    #     elif (scanStart + timedelta(minutes=scanMins*26-bufferMins)) <= row <= (scanEnd + timedelta(minutes=scanMins*26+bufferMins)):\n",
    "    #         scanNum.append('14')\n",
    "    #     elif (scanStart + timedelta(minutes=scanMins*28-bufferMins)) <= row <= (scanEnd + timedelta(minutes=scanMins*28+bufferMins)):\n",
    "    #         scanNum.append('15')\n",
    "    #     elif (scanStart + timedelta(minutes=scanMins*30-bufferMins)) <= row <= (scanEnd + timedelta(minutes=scanMins*30+bufferMins)):\n",
    "    #         scanNum.append('16')\n",
    "\n",
    "    #     # If no times fit, apply N/A / Se nenhum tempo se encaixar, aplique N/A\n",
    "    #     else:\n",
    "    #         scanNum.append('') \n",
    "\n",
    "    # # Apply scan ID list to the dataframe / Aplicar lista de IDs de varredura ao dataframe\n",
    "    # df['scan'] = scanNum\n",
    "\n",
    "    # Run the observations method in spatialFunctions / Execute o método de observações em spaceFunctions\n",
    "    observations(df)\n",
    "\n",
    "    # Make geographic and set CRS / Faça geográfica e defina CRS\n",
    "    gdf = gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(df.lon, df.lat)) \n",
    "    gdf = gdf.set_crs('EPSG:4326')\n",
    "    gdf = gdf.to_crs('EPSG:31985')\n",
    "\n",
    "    # Check and create save directory for gpkg files / Verifique e crie um diretório de salvamento para arquivos gpkg\n",
    "    gpkgsavePath = './gpkgData'\n",
    "    isDir = os.path.isdir(gpkgsavePath)\n",
    "    if isDir == False:\n",
    "        mkdir('gpkgData')\n",
    "    \n",
    "    # Export gdf into gpkg / Exportar gdf para gpkg\n",
    "    gdf.to_file('gpkgData/'+i[:-4]+'scans.gpkg', driver=\"GPKG\", layer=i[:-4]+'_wholeDay')\n",
    "    \n",
    "    # Export each scan as a separate layer using the scanSpatial method in spatialFunctions\n",
    "    # Exporte cada varredura como uma camada separada usando o método scanSpatial em spaceFunctions\n",
    "    spatialCounter = '1'\n",
    "    gdfs1 = gdf[(gdf['scan'].isin(['1']))]\n",
    "    if not gdfs1.empty:\n",
    "        scanSpatial(gdfs1, i, '1')\n",
    "\n",
    "    gdfs2 = gdf[(gdf['scan'].isin(['2']))]\n",
    "    if not gdfs2.empty:\n",
    "        scanSpatial(gdfs2, i, '2')\n",
    "\n",
    "    gdfs3 = gdf[(gdf['scan'].isin(['3']))]\n",
    "    if not gdfs3.empty:\n",
    "        scanSpatial(gdfs3, i, '3')\n",
    "\n",
    "    gdfs4 = gdf[(gdf['scan'].isin(['4']))]\n",
    "    if not gdfs4.empty:\n",
    "        scanSpatial(gdfs4, i, '4')\n",
    "\n",
    "    gdfs5 = gdf[(gdf['scan'].isin(['5']))]\n",
    "    if not gdfs5.empty:\n",
    "        scanSpatial(gdfs5, i, '5')\n",
    "\n",
    "    gdfs6 = gdf[(gdf['scan'].isin(['6']))]\n",
    "    if not gdfs6.empty:\n",
    "        scanSpatial(gdfs6, i, '6')\n",
    "\n",
    "    gdfs7 = gdf[(gdf['scan'].isin(['7']))]\n",
    "    if not gdfs7.empty:\n",
    "            scanSpatial(gdfs7, i, '7')\n",
    "\n",
    "    gdfs8 = gdf[(gdf['scan'].isin(['8']))]\n",
    "    if not gdfs8.empty:\n",
    "        scanSpatial(gdfs8, i, '8')\n",
    "\n",
    "    gdfs9 = gdf[(gdf['scan'].isin(['9']))]\n",
    "    if not gdfs9.empty:\n",
    "        scanSpatial(gdfs9, i, '9')\n",
    "\n",
    "    gdfs10 = gdf[(gdf['scan'].isin(['10']))]\n",
    "    if not gdfs10.empty:\n",
    "        scanSpatial(gdfs10, i, '10')\n",
    "\n",
    "    # Create layers for non-scan data / Crie camadas para dados não digitalizados\n",
    "    gdfago = gdf[(gdf['scan'].isin(['ago']))]\n",
    "    if not gdfago.empty:\n",
    "        gdfago.to_file('gpkgData/'+i[:-4]+'scans.gpkg', driver=\"GPKG\", layer=i[:-4]+'_ago')\n",
    "\n",
    "    gdfother = gdf[(gdf['scan'].isin(['other']))]\n",
    "    if not gdfother.empty:\n",
    "        gdfother.to_file('gpkgData/'+i[:-4]+'scans.gpkg', driver=\"GPKG\", layer=i[:-4]+'_other')\n",
    "\n",
    "    # Check and create save directory for csv files / Verifique e crie um diretório de salvamento para arquivos csv\n",
    "    csvsavePath = './csvDayFiles'\n",
    "    isDir = os.path.isdir(csvsavePath)\n",
    "    if isDir == False:\n",
    "        mkdir('csvDayFiles')\n",
    "    \n",
    "    # Save to csv / Salvar em csv\n",
    "    gdf.to_csv('csvDayFiles/'+i[:-4]+'.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "        # Calculate the centroid of the group\n",
    "        centroid = gdfs1.dissolve().centroid\n",
    "        \n",
    "        # Calculate distance of each point to the centroid of the group\n",
    "        for row in gdfs1['geometry']:\n",
    "            gdfs1.loc[:,'distCentr'] = gdfs1.distance(centroid[0])\n",
    "\n",
    "        # Create geodataframe for the area, perimeter, and polygon of each scan\n",
    "        area = gdfs1.dissolve().convex_hull\n",
    "        area = gpd.GeoDataFrame(gpd.GeoSeries(area))\n",
    "        area = area.rename(columns={0:'geometry'}).set_geometry('geometry')\n",
    "        area.loc[:,'area'] = area.area\n",
    "        area.loc[:,'perimeter'] = area.length\n",
    "        centroid.to_file('gpkgData/'+i[:-4]+'scans.gpkg', driver=\"GPKG\", layer=i[:-4]+'_scan1_centroid')\n",
    "        area.to_file('gpkgData/'+i[:-4]+'scans.gpkg', driver=\"GPKG\", layer=i[:-4]+'_scan1_area')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing using gdf's for each scan in above loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the centroid of the group\n",
    "centroid = gdfs1.dissolve().centroid\n",
    "centroid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress warning for setting with copy, non-issue here\n",
    "pd.set_option('mode.chained_assignment',None)\n",
    "\n",
    "# Calculate distance of each point to the centroid of the group\n",
    "for row in gdfs1['geometry']:\n",
    "    gdfs1.loc[:,'distCentr'] = gdfs1.distance(centroid[0])\n",
    "gdfs1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create geodataframe for the area, perimeter, and polygon of each scan\n",
    "area = gdfs1.dissolve().convex_hull\n",
    "area = gpd.GeoDataFrame(gpd.GeoSeries(area))\n",
    "area = area.rename(columns={0:'geometry'}).set_geometry('geometry')\n",
    "area.loc[:,'area'] = area.area\n",
    "area.loc[:,'perimeter'] = area.length\n",
    "area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "area.to_file('gpkgData/20220612scans.gpkg', driver=\"GPKG\", layer='area')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centroid.to_file('gpkgData/20220612scans.gpkg', driver=\"GPKG\", layer='centroid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Section 2\n",
    "Analysis of scans, analyze all of this for every individual scan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For GPKG data stored in gpkgData folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "\n",
    "gpkgDict = dict()\n",
    "\n",
    "for file in os.listdir('gpkgData'):\n",
    "    if file.endswith('.gpkg'):\n",
    "       gpkgDict[file] = 'file_'+file\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# for i in gpkgDict:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distance between each centroid in temporal order\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subgroups/cluster analysis, find clusters on eah scan and distance from each sub-centroid to main group centroid\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Export Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export data frame to gpkg for use in QGIS / Exportar quadro de dados para gpkg para uso no QGIS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('venvScanAutomation': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f0f140c0515cce258fdf849486238554cd45128ff92344433aac6799452693c7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
